{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc14e1a7-458d-4b54-ad8f-d0664a305395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f735945a-bc72-4e5a-a303-2138f7f198c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Union, List, Optional\n",
    "\n",
    "def _wrap(x):\n",
    "    if isinstance(x, (int, float)): return Constant(x)\n",
    "    return x\n",
    "\n",
    "@dataclass\n",
    "class Expression:\n",
    "    # --- Arithmetic Overloads ---\n",
    "    def __add__(self, other): return BinaryOp(self, _wrap(other), \"+\")\n",
    "    def __radd__(self, other): return BinaryOp(_wrap(other), self, \"+\")\n",
    "    \n",
    "    def __sub__(self, other): return BinaryOp(self, _wrap(other), \"-\")\n",
    "    def __rsub__(self, other): return BinaryOp(_wrap(other), self, \"-\")\n",
    "    \n",
    "    def __mul__(self, other): return BinaryOp(self, _wrap(other), \"*\")\n",
    "    def __rmul__(self, other): return BinaryOp(_wrap(other), self, \"*\")\n",
    "    \n",
    "    def __truediv__(self, other): return BinaryOp(self, _wrap(other), \"/\")\n",
    "    def __rtruediv__(self, other): return BinaryOp(_wrap(other), self, \"/\")\n",
    "    \n",
    "    def __pow__(self, other): return BinaryOp(self, _wrap(other), \"**\")\n",
    "    \n",
    "    def __neg__(self): return UnaryOp(self, \"-\")\n",
    "\n",
    "@dataclass\n",
    "class Symbol(Expression): name: str\n",
    "@dataclass\n",
    "class Field(Symbol): rank: int = 0\n",
    "@dataclass\n",
    "class Parameter(Symbol): pass\n",
    "@dataclass\n",
    "class Constant(Expression): value: float\n",
    "\n",
    "@dataclass\n",
    "class UnaryOp(Expression):\n",
    "    operand: Expression; op: str\n",
    "\n",
    "@dataclass\n",
    "class BinaryOp(Expression):\n",
    "    left: Expression; right: Expression; op: str\n",
    "\n",
    "@dataclass\n",
    "class Equation:\n",
    "    lhs: Expression; rhs: Expression\n",
    "    def __repr__(self): return f\"{self.lhs} == {self.rhs}\"\n",
    "\n",
    "# Functional Helpers\n",
    "def dt(expr): return UnaryOp(expr, 'dt')\n",
    "def grad(expr): return UnaryOp(expr, 'grad')\n",
    "def div(expr): return UnaryOp(expr, 'div')\n",
    "def laplacian(expr): return UnaryOp(expr, 'laplacian')\n",
    "def Eq(lhs, rhs): return Equation(lhs, rhs)\n",
    "def vec(expr): return UnaryOp(expr, 'vec') # Explicit vector construction if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62c6dc8e-b48a-4761-94e7-a1c1826176ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "#from dsl_core import *\n",
    "\n",
    "class JAXCompiler:\n",
    "    def __init__(self, algebra, manifold):\n",
    "        self.algebra = algebra\n",
    "        self.manifold = manifold\n",
    "        self.state_registry = {}\n",
    "        self.param_registry = {}\n",
    "\n",
    "    def compile(self, equation: Equation):\n",
    "        # 1. Residual Form (LHS - RHS)\n",
    "        if not (isinstance(equation.lhs, UnaryOp) and equation.lhs.op == 'dt'):\n",
    "             return self._compile_residual(equation)\n",
    "        \n",
    "        # 2. Update Form (dt(u) = RHS)\n",
    "        rhs_expr = equation.rhs\n",
    "        def kernel(u_dict, params_dict):\n",
    "            self.state_registry = u_dict\n",
    "            self.param_registry = params_dict\n",
    "            return self._eval(rhs_expr)\n",
    "        return kernel\n",
    "\n",
    "    def _compile_residual(self, equation):\n",
    "        def residual_fn(u_dict, params_dict):\n",
    "            self.state_registry = u_dict\n",
    "            self.param_registry = params_dict\n",
    "            return self._eval(equation.lhs) - self._eval(equation.rhs)\n",
    "        return residual_fn\n",
    "\n",
    "    def _contract_list(self, val_list):\n",
    "        \"\"\" Converts [v_x, v_y] -> Sum(e_i * v_i) for Geometric Algebra \"\"\"\n",
    "        if self.algebra is None: return val_list[0]\n",
    "        acc = jnp.zeros_like(val_list[0])\n",
    "        for i, comp in enumerate(val_list):\n",
    "            if i >= len(self.algebra.basis_matrices): break\n",
    "            M = self.algebra.basis_matrices[i]\n",
    "            term = jnp.einsum('kj,...j->...k', M, comp)\n",
    "            acc = acc + term\n",
    "        return acc\n",
    "\n",
    "    def _eval(self, expr: Expression):\n",
    "        # --- ATOMS ---\n",
    "        if isinstance(expr, Field): return self.state_registry[expr.name]\n",
    "        if isinstance(expr, Parameter): return self.param_registry[expr.name]\n",
    "        if isinstance(expr, Constant): return expr.value\n",
    "\n",
    "        # --- BINARY OPS ---\n",
    "        if isinstance(expr, BinaryOp):\n",
    "            left = self._eval(expr.left)\n",
    "            right = self._eval(expr.right)\n",
    "            is_list_l, is_list_r = isinstance(left, list), isinstance(right, list)\n",
    "\n",
    "            if expr.op == '+': \n",
    "                if is_list_l and not is_list_r: return self._contract_list(left) + right\n",
    "                if not is_list_l and is_list_r: return left + self._contract_list(right)\n",
    "                if is_list_l and is_list_r: return [l+r for l,r in zip(left, right)]\n",
    "                return left + right\n",
    "            \n",
    "            if expr.op == '-':\n",
    "                if is_list_l and not is_list_r: return self._contract_list(left) - right\n",
    "                if not is_list_l and is_list_r: return left - self._contract_list(right)\n",
    "                if is_list_l and is_list_r: return [l-r for l,r in zip(left, right)]\n",
    "                return left - right\n",
    "\n",
    "            if expr.op == '*': return self._apply_product(left, right)\n",
    "            if expr.op == '/': return left / right if not is_list_l else [l/right for l in left]\n",
    "            if expr.op == '**': return left ** right\n",
    "\n",
    "        # --- UNARY OPS ---\n",
    "        if isinstance(expr, UnaryOp):\n",
    "            val = self._eval(expr.operand)\n",
    "            \n",
    "            if expr.op == '-': \n",
    "                return [-v for v in val] if isinstance(val, list) else -val\n",
    "            \n",
    "            # --- GRADIENT (Central Difference) ---\n",
    "            if expr.op == 'grad':\n",
    "                # Manifold returns (fwd, bwd, lap)\n",
    "                gf, gb, _ = self.manifold.gradients(val)\n",
    "                # Average to get Central Difference: (Next - Prev) / 2dx\n",
    "                return [0.5 * (f + b) for f, b in zip(gf, gb)]\n",
    "            \n",
    "            # --- LAPLACIAN ---\n",
    "            if expr.op == 'laplacian':\n",
    "                _, _, lap = self.manifold.gradients(val)\n",
    "                return lap\n",
    "\n",
    "            # --- DIVERGENCE (Restored!) ---\n",
    "            if expr.op == 'div':\n",
    "                if isinstance(val, list):\n",
    "                    # val is a list of components [Fx, Fy]\n",
    "                    # div = d(Fx)/dx + d(Fy)/dy\n",
    "                    div_sum = 0\n",
    "                    for i, comp in enumerate(val):\n",
    "                        # Get Central Gradient of component i\n",
    "                        gf, gb, _ = self.manifold.gradients(comp)\n",
    "                        grad_central = 0.5 * (gf[i] + gb[i]) # Select derivative along axis i\n",
    "                        div_sum += grad_central\n",
    "                    return div_sum\n",
    "                else:\n",
    "                    # Fallback if someone asks for div(scalar) -> Laplacian\n",
    "                    _, _, lap = self.manifold.gradients(val)\n",
    "                    return lap\n",
    "\n",
    "        raise NotImplementedError(f\"Op {expr} not implemented\")\n",
    "\n",
    "    def _apply_product(self, left, right):\n",
    "        is_list_l, is_list_r = isinstance(left, list), isinstance(right, list)\n",
    "        if is_list_r and not is_list_l: return [left * r for r in right]\n",
    "        if is_list_l and not is_list_r: return [l * right for l in left]\n",
    "        if is_list_l and is_list_r:\n",
    "            # Dot Product accumulation\n",
    "            acc = 0\n",
    "            for l, r in zip(left, right): acc += l * r\n",
    "            return acc\n",
    "        return left * right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d631dd7f-1dda-44d2-8ed1-1b321d3fbaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonUniformMesh1D:\n",
    "    def __init__(self, L_total, N_points):\n",
    "        # Create non-uniform grid using tanh or geometric progression\n",
    "        # Dense in middle (x=0), sparse at ends (-L/2, L/2)\n",
    "        xi = jnp.linspace(-1, 1, N_points)\n",
    "        # Mapping function: x = L/2 * sign(xi) * |xi|^power\n",
    "        # Or Tanh stretching\n",
    "        k = 3.0 # Stretch factor\n",
    "        self.nodes = (L_total / 2) * (jnp.tanh(k * xi) / jnp.tanh(k)) + (L_total/2)\n",
    "        \n",
    "        # Pre-compute metric (dx)\n",
    "        # Central difference weights for non-uniform grid are complex.\n",
    "        # Simplification: Map to uniform Computational Space (xi) + Jacobian\n",
    "        \n",
    "        # Simple Finite Difference on irregular grid:\n",
    "        # dx_fwd[i] = x[i+1] - x[i]\n",
    "        self.dx_fwd = self.nodes[1:] - self.nodes[:-1]\n",
    "        self.dx_bwd = jnp.concatenate([self.dx_fwd[0:1], self.dx_fwd]) # Shifted\n",
    "        self.dx_fwd = jnp.concatenate([self.dx_fwd, self.dx_fwd[-1:]]) # Pad\n",
    "        \n",
    "    def gradients(self, u):\n",
    "        # u: (N,) or (N, 1)\n",
    "        \n",
    "        # Forward Diff\n",
    "        u_next = jnp.roll(u, -1, axis=0)\n",
    "        u_next = u_next.at[-1].set(u[-1]) # Clamp boundary\n",
    "        grad_f = (u_next - u) / self.dx_fwd\n",
    "        \n",
    "        # Backward Diff\n",
    "        u_prev = jnp.roll(u, 1, axis=0)\n",
    "        u_prev = u_prev.at[0].set(u[0])\n",
    "        grad_b = (u - u_prev) / self.dx_bwd\n",
    "        \n",
    "        # Laplacian (Non-uniform 3-point stencil)\n",
    "        # 2 * ( (u+ - u)/h1 - (u - u-)/h2 ) / (h1 + h2)\n",
    "        numer = (u_next - u)/self.dx_fwd - (u - u_prev)/self.dx_bwd\n",
    "        denom = 0.5 * (self.dx_fwd + self.dx_bwd)\n",
    "        lap = numer / denom\n",
    "        \n",
    "        return [grad_f], [grad_b], lap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9033304d-45fa-4b51-97be-2913f2d2556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import sparse\n",
    "\n",
    "class UnstructuredMesh2D:\n",
    "    def __init__(self, vertices, faces):\n",
    "        \"\"\"\n",
    "        vertices: (N, 2) array of [x, y]\n",
    "        faces: (M, 3) array of [n1, n2, n3] indices\n",
    "        \"\"\"\n",
    "        self.nodes = vertices\n",
    "        self.faces = faces\n",
    "        self.N = len(vertices)\n",
    "        \n",
    "        print(f\"Building Unstructured Operators for {self.N} nodes...\")\n",
    "        self.Gx, self.Gy, self.L = self._build_operators(vertices, faces)\n",
    "\n",
    "    def gradients(self, u):\n",
    "        # Maps generic DSL call 'gradients(u)' to our matrix ops\n",
    "        # u shape: (N,)\n",
    "        \n",
    "        # Gradient is a vector [d/dx, d/dy]\n",
    "        # In JAX sparse, matmul is distinct\n",
    "        dx = self.Gx @ u\n",
    "        dy = self.Gy @ u\n",
    "        \n",
    "        # Laplacian\n",
    "        lap = self.L @ u\n",
    "        \n",
    "        # Return format: ([grad_components...], [grad_components_bwd...], lap)\n",
    "        # For unstructured, we don't distinguish fwd/bwd in this simple reconstruction\n",
    "        grads = [dx, dy]\n",
    "        return (grads, grads, lap)\n",
    "\n",
    "    def _build_operators(self, V, F):\n",
    "        # 1. Compute Triangle Gradients\n",
    "        # For a triangle with coords (x1,y1), (x2,y2), (x3,y3)\n",
    "        # and values u1, u2, u3.\n",
    "        # The gradient is constant. \n",
    "        # Area = 0.5 * det(...)\n",
    "        \n",
    "        # Vectorized implementation for M triangles\n",
    "        v1 = V[F[:, 0]]; v2 = V[F[:, 1]]; v3 = V[F[:, 2]]\n",
    "        \n",
    "        x1, y1 = v1.T; x2, y2 = v2.T; x3, y3 = v3.T\n",
    "        \n",
    "        # 2A = (x2-x1)(y3-y1) - (x3-x1)(y2-y1)\n",
    "        two_area = (x2 - x1)*(y3 - y1) - (x3 - x1)*(y2 - y1)\n",
    "        area = 0.5 * two_area\n",
    "        \n",
    "        # Gradients of Basis functions (Linear Elements)\n",
    "        # b1_x = (y2 - y3) / 2A\n",
    "        # b1_y = (x3 - x2) / 2A\n",
    "        b1_x = (y2 - y3) / two_area; b1_y = (x3 - x2) / two_area\n",
    "        b2_x = (y3 - y1) / two_area; b2_y = (x1 - x3) / two_area\n",
    "        b3_x = (y1 - y2) / two_area; b3_y = (x2 - x1) / two_area\n",
    "        \n",
    "        # 2. Distribute Triangle Gradients to Nodes (Weighted Average by Area)\n",
    "        # This is the \"Lumped Mass\" approach.\n",
    "        # Node Gradient = sum(Area_tri * Grad_tri) / sum(Area_tri)\n",
    "        # Actually Area cancels out: sum(Area * (1/2A * ...)) = 1/2 * sum(...)\n",
    "        \n",
    "        # We build the Sparse Matrix 'G' directly.\n",
    "        # Rows = Nodes, Cols = Nodes.\n",
    "        # G_ij contributes to gradient at node i from neighbor j.\n",
    "        \n",
    "        # Easier approach for JAX: \n",
    "        # G maps (N_nodes) -> (N_nodes).\n",
    "        # We iterate triangles and accumulate contributions.\n",
    "        \n",
    "        # Init dense for simplicity (N < 500), convert to sparse later\n",
    "        # (For PhD scale, you'd build Coordinate list (row, col, val))\n",
    "        N = len(V)\n",
    "        Gx = jnp.zeros((N, N))\n",
    "        Gy = jnp.zeros((N, N))\n",
    "        Mass = jnp.zeros(N) # Sum of areas touching a node\n",
    "        \n",
    "        # It's cleaner to define the operators in pure numpy first, then move to JAX\n",
    "        # to avoid slow JAX loops during setup.\n",
    "        import numpy as np\n",
    "        Gx_np = np.zeros((N, N)); Gy_np = np.zeros((N, N)); L_np = np.zeros((N, N))\n",
    "        Mass_np = np.zeros(N)\n",
    "        \n",
    "        # Loop over faces (slow in Python, but done once)\n",
    "        # Optimization: This is standard FEM assembly.\n",
    "        for i in range(len(F)):\n",
    "            idx = F[i] # [n1, n2, n3]\n",
    "            a = area[i]\n",
    "            \n",
    "            # Gradients of shape functions\n",
    "            # dN/dx = [b1_x, b2_x, b3_x]\n",
    "            # Grad_Tri = dN/dx . U_element\n",
    "            bx = [b1_x[i], b2_x[i], b3_x[i]]\n",
    "            by = [b1_y[i], b2_y[i], b3_y[i]]\n",
    "            \n",
    "            # Accumulate to all 3 nodes of this triangle\n",
    "            for row_node in idx:\n",
    "                Mass_np[row_node] += a\n",
    "                for k, col_node in enumerate(idx):\n",
    "                    # Contribution of u[col_node] to Grad at row_node\n",
    "                    # Weighted by area: Area * (dN_k/dx)\n",
    "                    # Note: Area * b = 0.5 * (coord diff)\n",
    "                    Gx_np[row_node, col_node] += a * bx[k]\n",
    "                    Gy_np[row_node, col_node] += a * by[k]\n",
    "                    \n",
    "            # Laplacian Stiffnes Matrix (Cotangent)\n",
    "            # Standard FEM: K_ij = Integral(grad_Ni . grad_Nj)\n",
    "            # Local K = Area * (bx_i*bx_j + by_i*by_j)\n",
    "            for r in range(3):\n",
    "                row = idx[r]\n",
    "                for c in range(3):\n",
    "                    col = idx[c]\n",
    "                    val = a * (bx[r]*bx[c] + by[r]*by[c])\n",
    "                    L_np[row, col] += val\n",
    "\n",
    "        # Normalize Gradients by Mass (Area)\n",
    "        # G = inv(M) * G_accum\n",
    "        inv_mass = 1.0 / (Mass_np + 1e-9)\n",
    "        Gx_np = (Gx_np.T * inv_mass).T\n",
    "        Gy_np = (Gy_np.T * inv_mass).T\n",
    "        \n",
    "        # Laplacian: FEM solves K u = F. \n",
    "        # But our code expects 'laplacian(u)' to return the value div(grad u).\n",
    "        # In Lumped mass approximation: L u = M^-1 K u\n",
    "        L_np = -1 * (L_np.T * inv_mass).T # Sign convention: laplacian is negative definite\n",
    "        \n",
    "        return jnp.array(Gx_np), jnp.array(Gy_np), jnp.array(L_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1caa3a25-c732-4adb-b4fd-38e8b789a428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, lax\n",
    "import functools\n",
    "from kingdon import Algebra\n",
    "from typing import NamedTuple, List\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. REUSED ABSTRACTIONS (The Framework)\n",
    "# ==============================================================================\n",
    "\n",
    "class MatrixAlgebra:\n",
    "    \"\"\" Pre-computes Geometric Product matrices (Works for 1D, 2D, 3D) \"\"\"\n",
    "    def __init__(self, p, q, r=0):\n",
    "        self.p, self.q, self.r = p, q, r\n",
    "        self.alg = Algebra(p, q, r)\n",
    "        self.dim = len(self.alg) # 1D=2 (Scalar, Vector), 2D=4\n",
    "        # Basis generators (e1...)\n",
    "        self.basis_names = [f'e{i+1}' for i in range(p + q + r)]\n",
    "        \n",
    "        matrices = []\n",
    "        for name in self.basis_names:\n",
    "            e_k = self.alg.blades[name]\n",
    "            cols = []\n",
    "            for i in range(self.dim):\n",
    "                b_i = self.alg.multivector({i: 1})\n",
    "                res = e_k * b_i\n",
    "                dense_col = jnp.zeros(self.dim)\n",
    "                for bin_key, val in res.items(): dense_col = dense_col.at[bin_key].set(val)\n",
    "                cols.append(dense_col)\n",
    "            matrices.append(jnp.stack(cols, axis=1))\n",
    "        self.basis_matrices = jnp.stack(matrices)\n",
    "\n",
    "class CartesianBox:\n",
    "    \"\"\" Generic N-Dimensional Grid with Boundary Support \"\"\"\n",
    "    def __init__(self, shape, dx):\n",
    "        self.shape = shape # Tuple (Nx, Ny...)\n",
    "        self.dx = dx\n",
    "        self.ndim = len(shape)\n",
    "        \n",
    "        # Create coordinates\n",
    "        coords = [jnp.linspace(0, s*dx, s) for s in shape]\n",
    "        self.grid = jnp.meshgrid(*coords, indexing='ij')\n",
    "\n",
    "    @property\n",
    "    def coordinates(self): return self.grid\n",
    "\n",
    "    def gradients(self, u):\n",
    "        # Generic padding for any dimension\n",
    "        # Pad spatial dims, leave channel dim (last) alone if it exists\n",
    "        # u shape is (N, N, ..., Channels)\n",
    "        \n",
    "        # Determine padding width: ((1,1), (1,1)... (0,0))\n",
    "        # We assume u has exactly self.ndim spatial dimensions + optional channel dim\n",
    "        has_channel = (u.ndim > self.ndim)\n",
    "        pad_width = [(1, 1)] * self.ndim\n",
    "        if has_channel: pad_width.append((0, 0))\n",
    "        \n",
    "        u_pad = jnp.pad(u, tuple(pad_width), mode='edge') \n",
    "        \n",
    "        center_slice = [slice(1, -1)] * self.ndim\n",
    "        if has_channel: center_slice.append(slice(None))\n",
    "        u_c = u_pad[tuple(center_slice)]\n",
    "        \n",
    "        grads_fwd = []\n",
    "        grads_bwd = []\n",
    "        lap_sum = jnp.zeros_like(u_c)\n",
    "        \n",
    "        for i in range(self.ndim):\n",
    "            # Slices for neighbor access along axis i\n",
    "            # Next neighbor\n",
    "            s_next = [slice(1, -1)] * self.ndim\n",
    "            s_next[i] = slice(2, None)\n",
    "            if has_channel: s_next.append(slice(None))\n",
    "            \n",
    "            # Prev neighbor\n",
    "            s_prev = [slice(1, -1)] * self.ndim\n",
    "            s_prev[i] = slice(0, -2)\n",
    "            if has_channel: s_prev.append(slice(None))\n",
    "            \n",
    "            u_next = u_pad[tuple(s_next)]\n",
    "            u_prev = u_pad[tuple(s_prev)]\n",
    "            \n",
    "            grads_fwd.append( (u_next - u_c) / self.dx )\n",
    "            grads_bwd.append( (u_c - u_prev) / self.dx )\n",
    "            lap_sum += (u_next + u_prev - 2*u_c) / (self.dx**2)\n",
    "            \n",
    "        return (grads_fwd, grads_bwd, lap_sum)\n",
    "\n",
    "    def enforce_boundaries(self, u):\n",
    "        \"\"\" Zero out the spatial boundaries (Hard Walls) \"\"\"\n",
    "        # Works for 1D, 2D, 3D...\n",
    "        res = u\n",
    "        for i in range(self.ndim):\n",
    "            # Create a dynamic slice object\n",
    "            # u.at[..., 0, ...].set(0) where 0 is at axis i\n",
    "            \n",
    "            # Left Boundary (Index 0)\n",
    "            sl_0 = [slice(None)] * u.ndim\n",
    "            sl_0[i] = 0\n",
    "            res = res.at[tuple(sl_0)].set(0.0)\n",
    "            \n",
    "            # Right Boundary (Index -1)\n",
    "            sl_1 = [slice(None)] * u.ndim\n",
    "            sl_1[i] = -1\n",
    "            res = res.at[tuple(sl_1)].set(0.0)\n",
    "            \n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34e49a35-f68c-4f7c-bca6-4dc24620b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCADSolverComposer:\n",
    "    def __init__(self, op_factory):\n",
    "        self.ops = op_factory\n",
    "\n",
    "    def compile_hybrid_maccormack(self, eq_advection, eq_diffusion):\n",
    "        # Explicit Advection Operators\n",
    "        adv_fwd = self.ops.build_functional(eq_advection, mode='fwd')\n",
    "        adv_bwd = self.ops.build_functional(eq_advection, mode='bwd')\n",
    "        \n",
    "        # Implicit Diffusion Operator Factory\n",
    "        diff_matrix_factory = self.ops.build_matrix(eq_diffusion)\n",
    "        \n",
    "        def step_fn(u_curr, params, dt):\n",
    "            # 1. MacCormack Advection\n",
    "            k1 = adv_fwd(u_curr, params)\n",
    "            u_p = u_curr + k1 * dt\n",
    "            \n",
    "            k2 = adv_bwd(u_p, params)\n",
    "            u_adv = 0.5 * (u_curr + u_p + k2 * dt)\n",
    "            \n",
    "            # Note: Diffusion is applied via matrix multiply outside this step\n",
    "            # to keep the JIT loop clean and modular.\n",
    "            return u_adv \n",
    "            \n",
    "        return step_fn, diff_matrix_factory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a555be0-7ac4-4eba-bce9-0ffc0819ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e138bcee-a99c-46d3-86d6-305c561bd14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Matrix-Free Hybrid Solver Compiled (Fixed) ---\n",
      "Generating Ground Truth...\n",
      "\n",
      "--- Starting Matrix-Free Precision Search ---\n",
      "\n",
      ">>> STAGE 0 (Epsilon=2.0) <<<\n",
      "  Iter   0: Loss=0.4141 | Vel=[-4.99991963  4.99993126] | V=0.500\n",
      "  Iter  20: Loss=0.0612 | Vel=[-70.4819555  76.3940099] | V=10.924\n",
      "  Iter  40: Loss=0.0105 | Vel=[-60.81338231  64.02255718] | V=15.404\n",
      "  Iter  60: Loss=0.0042 | Vel=[-56.05321931  64.85965103] | V=10.363\n",
      "  Iter  80: Loss=0.0025 | Vel=[-49.92660438  58.58254573] | V=3.798\n",
      "  Iter 100: Loss=0.0018 | Vel=[-40.8963796   51.15803643] | V=-0.503\n",
      "  Iter 120: Loss=0.0015 | Vel=[-32.03709638  44.26590873] | V=-2.163\n",
      "  Iter 140: Loss=0.0012 | Vel=[-23.66307176  37.50746745] | V=-2.396\n",
      "  Iter 160: Loss=0.0010 | Vel=[-15.59953386  30.87466563] | V=-2.272\n",
      "  Iter 180: Loss=0.0008 | Vel=[-7.92960878 24.49939463] | V=-2.165\n",
      "  Iter 200: Loss=0.0007 | Vel=[-0.77082548 18.48419117] | V=-2.107\n",
      "\n",
      ">>> STAGE 1 (Epsilon=0.5) <<<\n",
      "  Iter   0: Loss=0.0006 | Vel=[-0.42078298 18.19470447] | V=-2.096\n",
      "  Iter  20: Loss=0.0004 | Vel=[ 5.84315373 12.88731513] | V=-1.061\n",
      "  Iter  40: Loss=0.0003 | Vel=[11.19528969  8.26152399] | V=-0.296\n",
      "  Iter  60: Loss=0.0003 | Vel=[15.90053233  4.12291893] | V=-0.169\n",
      "  Iter  80: Loss=0.0002 | Vel=[20.06374715  0.39912632] | V=-0.223\n",
      "  Iter 100: Loss=0.0002 | Vel=[23.70797232 -2.91402464] | V=-0.237\n",
      "  Iter 120: Loss=0.0002 | Vel=[26.86532367 -5.83434561] | V=-0.218\n",
      "  Iter 140: Loss=0.0002 | Vel=[29.57584142 -8.38873299] | V=-0.201\n",
      "  Iter 160: Loss=0.0002 | Vel=[ 31.88000387 -10.60494465] | V=-0.191\n",
      "  Iter 180: Loss=0.0001 | Vel=[ 33.8200501 -12.5130919] | V=-0.182\n",
      "  Iter 200: Loss=0.0001 | Vel=[ 35.43824665 -14.14422699] | V=-0.175\n",
      "\n",
      ">>> STAGE 2 (Epsilon=0.0) <<<\n",
      "  Iter   0: Loss=0.0001 | Vel=[ 35.51862358 -14.21690842] | V=-0.170\n",
      "  Iter  20: Loss=0.0001 | Vel=[ 36.69579247 -15.38882605] | V=0.288\n",
      "  Iter  40: Loss=0.0001 | Vel=[ 37.46180632 -16.23538981] | V=0.505\n",
      "  Iter  60: Loss=0.0001 | Vel=[ 38.05975441 -16.93461084] | V=0.458\n",
      "  Iter  80: Loss=0.0001 | Vel=[ 38.53998355 -17.51849235] | V=0.432\n",
      "  Iter 100: Loss=0.0001 | Vel=[ 38.91705231 -17.999358  ] | V=0.440\n",
      "  Iter 120: Loss=0.0001 | Vel=[ 39.21230938 -18.39527721] | V=0.445\n",
      "  Iter 140: Loss=0.0001 | Vel=[ 39.44024483 -18.71947103] | V=0.445\n",
      "  Iter 160: Loss=0.0001 | Vel=[ 39.61381948 -18.98358439] | V=0.446\n",
      "  Iter 180: Loss=0.0001 | Vel=[ 39.7442696  -19.19772629] | V=0.447\n",
      "  [CONVERGED] Iter 190 | Vel Delta=0.0099\n",
      "  Current: Vel=[ 39.79623593 -19.28878493] | V=0.447\n",
      "\n",
      "--- FINAL RESULT ---\n",
      "Rec Vel: [ 39.79623593 -19.28878493]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, lax, value_and_grad\n",
    "import jax.scipy.sparse.linalg as spla \n",
    "import optax\n",
    "#from dsl_core import *\n",
    "#from sta_inverse_precision import MatrixAlgebra, CartesianBox \n",
    "\n",
    "# --- 1. MODULAR COMPONENTS ---\n",
    "\n",
    "class StencilProvider:\n",
    "    def __init__(self, manifold):\n",
    "        self.manifold = manifold\n",
    "    \n",
    "    def get_grads(self, u):\n",
    "        return self.manifold.gradients(u)\n",
    "\n",
    "class OperatorFactory:\n",
    "    def __init__(self, algebra, stencil):\n",
    "        self.algebra = algebra\n",
    "        self.stencil = stencil\n",
    "        self.bases = tuple(self.algebra.basis_matrices)\n",
    "\n",
    "    def build_functional_fused(self, terms):\n",
    "        \"\"\" \n",
    "        Returns a kernel that computes specific physics terms.\n",
    "        terms: list of strings ['adv', 'diff', 'decay']\n",
    "        \"\"\"\n",
    "        # We pre-bake the allowed terms into the closure\n",
    "        enable_adv = 'adv' in terms\n",
    "        enable_diff = 'diff' in terms\n",
    "        enable_decay = 'decay' in terms\n",
    "        \n",
    "        def kernel(u, grads, lap, params):\n",
    "            rate = jnp.zeros_like(u)\n",
    "            \n",
    "            # 1. Advection: -c * (Basis . Grad)\n",
    "            if enable_adv and 'c' in params:\n",
    "                 c = params['c']\n",
    "                 # Safety check: if grads is None, we can't compute advection\n",
    "                 if grads is not None:\n",
    "                     for i, basis in enumerate(self.bases):\n",
    "                         if i >= len(grads): break\n",
    "                         term = jnp.einsum('kj,...j->...k', basis, grads[i])\n",
    "                         rate = rate - c * term\n",
    "\n",
    "            # 2. Diffusion: eps * Lap\n",
    "            if enable_diff and 'epsilon' in params:\n",
    "                 rate = rate + params['epsilon'] * lap\n",
    "                 \n",
    "            # 3. Decay: -V * u\n",
    "            if enable_decay and 'V' in params:\n",
    "                 rate = rate - params['V'] * u\n",
    "                 \n",
    "            return rate\n",
    "        return kernel\n",
    "\n",
    "class MatrixFreeComposer:\n",
    "    def __init__(self, ops):\n",
    "        self.ops = ops\n",
    "\n",
    "    def compile_hybrid(self):\n",
    "        # Explicit Kernel: Only Advection\n",
    "        kern_adv = self.ops.build_functional_fused(terms=['adv'])\n",
    "        \n",
    "        # Implicit Kernel: Diffusion + Decay\n",
    "        kern_diff = self.ops.build_functional_fused(terms=['diff', 'decay'])\n",
    "        \n",
    "        def step_fn(u_curr, params, dt):\n",
    "            # --- PHASE 1: EXPLICIT MACCORMACK (Advection) ---\n",
    "            gf, gb, lap = self.ops.stencil.get_grads(u_curr)\n",
    "            \n",
    "            # Predict\n",
    "            k1 = kern_adv(u_curr, gf, lap, params)\n",
    "            u_p = u_curr + k1 * dt\n",
    "            \n",
    "            # Correct\n",
    "            gf_p, gb_p, lap_p = self.ops.stencil.get_grads(u_p)\n",
    "            k2 = kern_adv(u_p, gb_p, lap_p, params)\n",
    "            \n",
    "            u_adv = 0.5 * (u_curr + u_p + k2 * dt)\n",
    "            \n",
    "            # --- PHASE 2: IMPLICIT DIFFUSION (Matrix-Free) ---\n",
    "            \n",
    "            def linear_op(x_flat):\n",
    "                x = x_flat.reshape(u_curr.shape)\n",
    "                # We need Laplacian of x for diffusion\n",
    "                _, _, lap_x = self.ops.stencil.get_grads(x)\n",
    "                \n",
    "                # Evaluate Diffusion/Decay only (passed grads=None safely)\n",
    "                rate = kern_diff(x, None, lap_x, params) \n",
    "                \n",
    "                # Implicit Relation: (I - dt * DiffOp) x = b\n",
    "                res = x - dt * rate \n",
    "                return res.ravel()\n",
    "\n",
    "            # Solve Ax = b\n",
    "            u_flat_adv = u_adv.ravel()\n",
    "            \n",
    "            # Use u_adv as guess. \n",
    "            # If eps=0, linear_op is Identity, converges in 1 iter.\n",
    "            u_next_flat, _ = spla.cg(linear_op, u_flat_adv, x0=u_flat_adv, tol=1e-5, maxiter=20)\n",
    "            \n",
    "            return u_next_flat.reshape(u_curr.shape)\n",
    "            \n",
    "        return step_fn\n",
    "\n",
    "# --- 2. SETUP ---\n",
    "GRID_SIZE = 100\n",
    "DX = 0.1\n",
    "DT = 0.0001\n",
    "DURATION_STEPS = 1200\n",
    "\n",
    "algebra = MatrixAlgebra(2, 0)\n",
    "manifold = CartesianBox((GRID_SIZE, GRID_SIZE), DX)\n",
    "stencil = StencilProvider(manifold)\n",
    "ops = OperatorFactory(algebra, stencil)\n",
    "composer = MatrixFreeComposer(ops)\n",
    "\n",
    "# Compile (No expression needed, we use the 'terms' logic)\n",
    "step_fn = composer.compile_hybrid()\n",
    "\n",
    "print(\"--- Matrix-Free Hybrid Solver Compiled (Fixed) ---\")\n",
    "\n",
    "# --- 3. RUNNER ---\n",
    "@jax.jit\n",
    "def run_matrix_free(params_dict, hyper_params):\n",
    "    u = jnp.zeros((GRID_SIZE, GRID_SIZE, algebra.dim))\n",
    "    X, Y = manifold.coordinates\n",
    "    p = {**params_dict, **hyper_params}\n",
    "    x0, y0 = p['pos']; vx, vy = p['vel']\n",
    "    \n",
    "    def body(carry, i):\n",
    "        u_curr = carry\n",
    "        u_next = step_fn(u_curr, p, DT)\n",
    "        u_next = manifold.enforce_boundaries(u_next)\n",
    "        \n",
    "        t = i * DT\n",
    "        src = jnp.exp(-((X-(x0+vx*t))**2 + (Y-(y0+vy*t))**2)/4.5) * \\\n",
    "              jnp.exp(-(i-50)**2/800.0) * 100.0 * DT\n",
    "        u_next = u_next.at[..., 0].add(src)\n",
    "        return u_next, u_next[..., 0]\n",
    "\n",
    "    _, history = lax.scan(body, u, jnp.arange(DURATION_STEPS))\n",
    "    return history\n",
    "\n",
    "# --- 4. PRECISION SOLVE ---\n",
    "def solve_final_matrix_free():\n",
    "    TRUE_PARAMS = {'pos': jnp.array([3.0, 7.0]), 'vel': jnp.array([40.0, -20.0]), 'c': 343.0, 'V': 0.5}\n",
    "    \n",
    "    print(\"Generating Ground Truth...\")\n",
    "    true_hist = run_matrix_free(TRUE_PARAMS, {'epsilon': 0.0})\n",
    "    \n",
    "    SENSORS = jnp.array([[90, 10], [90, 90], [10, 10], [10, 90]])\n",
    "    obs_data = true_hist[:, SENSORS[:,0], SENSORS[:,1]]\n",
    "    \n",
    "    guess = {'pos': jnp.array([5.0, 5.0]), 'vel': jnp.array([0.0, 0.0]), 'c': 300.0, 'V': 0.0}\n",
    "    optimizer = optax.chain(optax.clip_by_global_norm(1.0), optax.multi_transform(\n",
    "        {'pos': optax.adam(0.1), 'vel': optax.adam(5.0), 'c': optax.adam(1.0), 'V': optax.adam(0.5)},\n",
    "        {'pos':'pos', 'vel':'vel', 'c':'c', 'V':'V'}))\n",
    "    opt_state = optimizer.init(guess)\n",
    "    \n",
    "    # Compiled Update Step\n",
    "    @jax.jit\n",
    "    def update(state, guess, eps):\n",
    "        def loss(p):\n",
    "            sim = run_matrix_free(p, {'epsilon': eps})\n",
    "            dat = sim[:, SENSORS[:,0], SENSORS[:,1]]\n",
    "            safe = 1e-6\n",
    "            sim_n = (dat - jnp.mean(dat, 0)) / (jnp.std(dat, 0) + safe)\n",
    "            obs_n = (obs_data - jnp.mean(obs_data, 0)) / (jnp.std(obs_data, 0) + safe)\n",
    "            pb = jnp.sum(jnp.maximum(0, -p['pos'])) + jnp.sum(jnp.maximum(0, p['pos'] - 10.0))\n",
    "            return (1.0 - jnp.mean(sim_n * obs_n)) + pb\n",
    "        l, g = value_and_grad(loss)(guess)\n",
    "        u, s = optimizer.update(g, state, guess)\n",
    "        return l, optax.apply_updates(guess, u), s\n",
    "\n",
    "    print(\"\\n--- Starting Matrix-Free Precision Search ---\")\n",
    "    \n",
    "    for stage, eps in enumerate([2.0, 0.5, 0.0]):\n",
    "        print(f\"\\n>>> STAGE {stage} (Epsilon={eps}) <<<\")\n",
    "        for i in range(201):\n",
    "            loss, new_guess, opt_state = update(opt_state, guess, eps)\n",
    "            \n",
    "            d_vel = jnp.linalg.norm(new_guess['vel'] - guess['vel'])\n",
    "            guess = new_guess\n",
    "            \n",
    "            if i > 20 and d_vel < 0.01:\n",
    "                print(f\"  [CONVERGED] Iter {i} | Vel Delta={d_vel:.4f}\")\n",
    "                print(f\"  Current: Vel={guess['vel']} | V={guess['V']:.3f}\")\n",
    "                break\n",
    "            if i % 20 == 0:\n",
    "                print(f\"  Iter {i:3d}: Loss={loss:.4f} | Vel={guess['vel']} | V={guess['V']:.3f}\")\n",
    "\n",
    "    return guess\n",
    "\n",
    "final = solve_final_matrix_free()\n",
    "print(\"\\n--- FINAL RESULT ---\")\n",
    "print(f\"Rec Vel: {final['vel']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa335a9-8e1c-4699-ae59-86ccebe94e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
